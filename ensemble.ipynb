{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.ndimage import zoom\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torchsummary import summary\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix\n",
    "\n",
    "torch.cuda.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the right data:\n",
    "\n",
    "The full dataset has been processed from it's original 350GB+ MRI dataset. The original csv file is filtered to include only 1.5T or 3.0T scans as they contain the most comprehensive sets of MRI scans of differing types (T1-weighted, T2, Bold, etc). \n",
    "\n",
    "The clinical dementia rating (cdr) with values (0, 0.5, 1, 2) represent 0 = absent; 0.5 = questionable; 1= present, but mild; 2 = moderate (reference: https://www.sciencedirect.com/topics/neuroscience/clinical-dementia-rating). They have been remapped from (0, 0.5, 1, 2) to (0, 1, 2, 3) due to issues processing it with Torch dataloader, leading to a missing class for 0.5 cdr.\n",
    "\n",
    "Based on each MRI session value (Label), if there are multiple values for the cdr then the scans are excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1379\n",
      "['d0129', 'd2430', 'd3132']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       d0129\n",
       "2       d2430\n",
       "3       d3132\n",
       "12      d0371\n",
       "15      d2340\n",
       "        ...  \n",
       "6193    d0148\n",
       "6194    d2526\n",
       "6195    d1566\n",
       "6216    d1717\n",
       "6217    d0407\n",
       "Name: file_tag, Length: 1379, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagnostic_file = glob('diagnosis.csv')[0]\n",
    "\n",
    "diagnostic_df = pd.read_csv(diagnostic_file)\n",
    "\n",
    "diagnostic_df = diagnostic_df[(diagnostic_df.Scanner == '3.0T') | (diagnostic_df.Scanner == '1.5T')]\n",
    "\n",
    "multiple_values = diagnostic_df.groupby('Label').filter(lambda group: group['cdr'].nunique() > 1)['Label'].unique()\n",
    "\n",
    "cdr_map = {0.0: 0, 0.5: 1, 1.0: 2, 2.0: 3}\n",
    "\n",
    "# Filter out rows with these values in column A\n",
    "filtered_diagnostic_df = diagnostic_df[~diagnostic_df['Label'].isin(multiple_values)]\n",
    "filtered_diagnostic_df = filtered_diagnostic_df.drop_duplicates(subset='Label')\n",
    "# filtered_diagnostic_df['cdr'] = filtered_diagnostic_df['cdr'].map(cdr_map)\n",
    "filtered_diagnostic_df['file_tag'] = filtered_diagnostic_df.Label.apply(lambda file_name: file_name[-5:])\n",
    "print(len(filtered_diagnostic_df))\n",
    "\n",
    "valid_mr_scans_ls = filtered_diagnostic_df.file_tag.to_list()\n",
    "print(valid_mr_scans_ls[:3])\n",
    "filtered_diagnostic_df.file_tag\n",
    "\n",
    "# filtered_diagnostic_df.to_csv('filtered_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bids\\\\sub-OAS30001\\\\ses-d0129\\\\anat\\\\sub-OAS30001_ses-d0129_run-01_T1w.nii.gz', 'bids\\\\sub-OAS30001\\\\ses-d0129\\\\anat\\\\sub-OAS30001_ses-d0129_run-02_T1w.nii.gz']\n"
     ]
    }
   ],
   "source": [
    "data_path = 'bids/' ## Change as needed\n",
    "\n",
    "def recursive_glob_with_filter(directory, filter_list):\n",
    "    # Use recursive glob pattern to find all files\n",
    "    all_files = glob(directory + '/**/*T1*nii.gz', recursive=True)\n",
    "    \n",
    "    # Filter files based on the presence of strings from filter_list in the file path\n",
    "    filtered_files = [file for file in all_files if any(filter_str in file for filter_str in filter_list)]\n",
    "    \n",
    "    return filtered_files\n",
    "\n",
    "file_ls = recursive_glob_with_filter(data_path, valid_mr_scans_ls)\n",
    "print(file_ls[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0                                          file_path  label\n",
      "0              0  D:/DL/oasis-scripts/download_scans/bids\\sub-OA...    0.0\n",
      "1              1  D:/DL/oasis-scripts/download_scans/bids\\sub-OA...    0.0\n",
      "2              2  D:/DL/oasis-scripts/download_scans/bids\\sub-OA...    0.0\n",
      "3              3  D:/DL/oasis-scripts/download_scans/bids\\sub-OA...    0.0\n",
      "4              4  D:/DL/oasis-scripts/download_scans/bids\\sub-OA...    0.0\n",
      "...          ...                                                ...    ...\n",
      "2991        2991  D:/DL/oasis-scripts/download_scans/bids\\sub-OA...    0.0\n",
      "2992        2992  D:/DL/oasis-scripts/download_scans/bids\\sub-OA...    1.0\n",
      "2993        2993  D:/DL/oasis-scripts/download_scans/bids\\sub-OA...    0.0\n",
      "2994        2994  D:/DL/oasis-scripts/download_scans/bids\\sub-OA...    0.5\n",
      "2995        2995  D:/DL/oasis-scripts/download_scans/bids\\sub-OA...    0.0\n",
      "\n",
      "[2996 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Function to extract tag from file path\n",
    "def extract_tag_from_file_path(file_path):\n",
    "    file_path = file_path.replace(data_path[:-1], '')\n",
    "    return file_path.split('\\\\')[2][-5:]\n",
    "\n",
    "try:\n",
    "    data_loader_df = pd.read_csv('data_loader_df.csv')\n",
    "except:\n",
    "    # Create a new DataFrame to store the results\n",
    "    data_loader_df = pd.DataFrame(columns=['file_path', 'label'])\n",
    "\n",
    "    # Iterate over the file list and find the corresponding CDR value\n",
    "    for file_path in file_ls:\n",
    "        tag = extract_tag_from_file_path(file_path)\n",
    "        cdr_value = filtered_diagnostic_df.loc[filtered_diagnostic_df['file_tag'] == tag, 'cdr'].values\n",
    "        if cdr_value.size > 0:  # Check if cdr_value is not empty\n",
    "            new_row = pd.DataFrame({'file_path': [file_path], 'label': [cdr_value[0]]})\n",
    "            data_loader_df = pd.concat([data_loader_df, new_row], ignore_index=True)\n",
    "            data_loader_df.to_csv('data_loader_df.csv')\n",
    "\n",
    "print(data_loader_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is to extract 2D slices from 3D volumetric MRI scans. It iterates through the T1-weighted scans and slices between indexes 100-160, choosing a slice for every 3 slices. If the scan has less then 160 slices, then the middle slice will be taken of which there is only 1. Eventually there should be a dataset of 58,376. \n",
    "\n",
    "JPG is the saved file format due to its smaller size. NPY was tried but was 840GB and hence rejected.\n",
    "\n",
    "**Run only once (roughly 7-20 mins depending on CPU).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to an extremely imbalanced dataset (Label 0.0: 50216; Label 1.0: 5748; Label 2.0: 1971; Label 3.0: 441), weights will be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0.0: 50216\n",
      "Label 1.0: 5748\n",
      "Label 2.0: 1971\n",
      "Label 3.0: 441\n",
      "{0.0: 1.1624980086028358, 1.0: 10.155880306193458, 2.0: 29.617453069507864, 3.0: 132.3718820861678}\n",
      "cuda\n",
      "tensor([  1.1625,  10.1559,  29.6175, 132.3719])\n",
      "7.153921568627451\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "save_dir = data_path[:-5] + \"preprocessed_images\"\n",
    "# Dictionary to store the count of each label\n",
    "class_counts = defaultdict(int)\n",
    "\n",
    "# List to store file paths and labels\n",
    "file_paths_labels = []\n",
    "\n",
    "# Iterate over all files in the save directory\n",
    "for filename in os.listdir(save_dir):\n",
    "    if filename.startswith(\"label_\") and filename.endswith(\".jpg\"):\n",
    "        # Extract the label from the filename and convert it to a decimal (float)\n",
    "        label = float(filename.split(\"_\")[1])\n",
    "        # Increment the count for this label\n",
    "        class_counts[label] += 1\n",
    "        # Append the full file path and label to the list\n",
    "        full_path = os.path.join(save_dir, filename)\n",
    "        file_paths_labels.append((full_path, label))\n",
    "\n",
    "dementia_count = 0\n",
    "healthy_count = 0\n",
    "# Print the count of each label\n",
    "for label, count in class_counts.items():\n",
    "    print(f\"Label {label}: {count}\")\n",
    "    if label >0:\n",
    "        dementia_count+=count\n",
    "    else:\n",
    "        healthy_count+=count\n",
    "# Create a DataFrame from the list of file paths and labels\n",
    "jpg_data_loader_df = pd.DataFrame(file_paths_labels, columns=['file_path', 'label'])\n",
    "\n",
    "## Print the first few rows of the DataFrame\n",
    "# print(jpg_data_loader_df.head())\n",
    "\n",
    "# Calculate weights: Inverse of the frequency seems like a simple choice\n",
    "total_count = sum(class_counts.values())\n",
    "weights = {k: total_count / v for k, v in class_counts.items()}\n",
    "print(weights)\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# Convert weights to a tensor, ensuring the labels are in the correct order\n",
    "weights_tensor = torch.tensor([weights[0], weights[1], weights[2], weights[3]], dtype=torch.float32)\n",
    "\n",
    "print(weights_tensor)\n",
    "weights_tensor = weights_tensor.to(device)\n",
    "jpg_data_loader_df.to_csv('jpg_data_loader_df.csv')\n",
    "\n",
    "all_dementia_weight = (healthy_count+dementia_count)/dementia_count\n",
    "# all_dementia_weight = 3\n",
    "print(all_dementia_weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "Functions for training the model. Includes NiftiDataset, HierarchicalCrossEntropyLoss, load_model, train_model and evaluate_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0, 1.0, 2.0, 3.0}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "class NiftiDataset(Dataset):\n",
    "    def __init__(self, dataframe, preprocessed_dir):\n",
    "        self.dataframe = dataframe\n",
    "        self.preprocessed_dir = preprocessed_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        label = row['label']\n",
    "        img_path = row['file_path']  # Use the exact path from the dataframe\n",
    "        scan = cv2.imread(img_path)\n",
    "        # scan = cv2.resize(scan, (224,224), interpolation=cv2.INTER_LINEAR)\n",
    "        scan = scan / 255.0  # Normalize the image to [0, 1]\n",
    "        scan_tensor = torch.from_numpy(scan).float()\n",
    "        \n",
    "        # If the image is grayscale, we use unsqueeze to add the channel dimension\n",
    "        if len(scan.shape) == 2:\n",
    "            scan_tensor = scan_tensor.unsqueeze(0)  # Add channel dimension for grayscale image\n",
    "        else:\n",
    "            scan_tensor = scan_tensor.permute(2, 0, 1)  # Rearrange dimensions for color image\n",
    "        \n",
    "        label_tensor = torch.tensor(label).long()\n",
    "\n",
    "        return scan_tensor, label_tensor\n",
    "\n",
    "dataset = NiftiDataset(jpg_data_loader_df, preprocessed_dir=save_dir)\n",
    "\n",
    "# Create a list of labels for stratified splitting\n",
    "labels = jpg_data_loader_df.iloc[:, 1].values\n",
    "myset = set(labels)\n",
    "print(myset)\n",
    "\n",
    "# Define the ratios for splitting\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "# Calculate the sizes for each split\n",
    "total_size = len(dataset)\n",
    "train_size = int(total_size * train_ratio)\n",
    "val_size = int(total_size * val_ratio)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_dataset, val_test_dataset = random_split(dataset, [train_size, val_size + test_size])\n",
    "val_dataset, test_dataset = random_split(val_test_dataset, [val_size, test_size])\n",
    "\n",
    "# Create dataloaders for each split\n",
    "batch_size = 32  # Adjust the batch size as needed\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Function to count labels in a dataset\n",
    "def count_labels(dataset):\n",
    "    label_counts = {}\n",
    "    for _, label_tensor in dataset:\n",
    "        label = label_tensor.item()\n",
    "        if label in label_counts:\n",
    "            label_counts[label] += 1\n",
    "        else:\n",
    "            label_counts[label] = 1\n",
    "    return label_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def evaluate_ensemble(models, test_dataloader, device, method='averaging', weights=None):\n",
    "    predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    if method in ['weighted_ensemble', 'weighted_max_voting']:\n",
    "        if weights is None:\n",
    "            # Initialize weights with equal values\n",
    "            weights = [1.0 / len(models)] * len(models)\n",
    "        elif len(weights) != len(models):\n",
    "            raise ValueError(\"Number of weights must be equal to the number of models.\")\n",
    "\n",
    "    for inputs, labels in test_dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        if method == 'averaging':\n",
    "            ensemble_outputs = []\n",
    "            for i, model in enumerate(models):\n",
    "                model.cuda()\n",
    "                model.eval()\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                if hasattr(outputs, 'aux_logits'):\n",
    "                    outputs = outputs.logits\n",
    "\n",
    "                ensemble_outputs.append(outputs)\n",
    "\n",
    "            ensemble_outputs = torch.stack(ensemble_outputs, dim=0)\n",
    "            ensemble_mean = torch.mean(ensemble_outputs, dim=0)\n",
    "            _, predicted = torch.max(ensemble_mean, dim=1)\n",
    "        elif method == 'max_voting':\n",
    "            ensemble_votes = torch.zeros((len(models), len(inputs)), dtype=torch.long).to(device)\n",
    "            for i, model in enumerate(models):\n",
    "                model.cuda()\n",
    "                model.eval()\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                if hasattr(outputs, 'aux_logits'):\n",
    "                    outputs = outputs.logits\n",
    "\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                ensemble_votes[i] = predicted\n",
    "\n",
    "            predicted, _ = torch.mode(ensemble_votes, dim=0)\n",
    "        elif method == 'weighted_averaging':\n",
    "            ensemble_outputs = []\n",
    "            for i, model in enumerate(models):\n",
    "                model.cuda()\n",
    "                model.eval()\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                if hasattr(outputs, 'aux_logits'):\n",
    "                    outputs = outputs.logits\n",
    "\n",
    "                outputs *= weights[i]\n",
    "                ensemble_outputs.append(outputs)\n",
    "\n",
    "            ensemble_mean = torch.mean(torch.stack(ensemble_outputs, dim=0), dim=0)\n",
    "            _, predicted = torch.max(ensemble_mean, dim=1)\n",
    "            \n",
    "\n",
    "        predictions.extend(predicted.tolist())\n",
    "        all_labels.extend(labels.tolist())\n",
    "\n",
    "    cm = confusion_matrix(all_labels, predictions)\n",
    "    print(cm)\n",
    "    class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "    print(class_accuracies)\n",
    "    \n",
    "    accuracy = torch.sum(torch.tensor(predictions) == torch.tensor(all_labels)).item() / len(predictions)\n",
    "    f1 = f1_score(all_labels, predictions, average='macro')\n",
    "    precision = precision_score(all_labels, predictions, average='macro')\n",
    "    recall = recall_score(all_labels, predictions, average='macro')\n",
    "    \n",
    "    print(f'Test Accuracy: {accuracy:.4f}, Test F1: {f1:.4f}, Test Precision: {precision:.4f}, Test Recall: {recall:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load models from checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet = models.resnet18()\n",
    "num_features = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(num_features, 4)\n",
    "\n",
    "checkpoint = torch.load(\"checkpoints\\\\best_resnet18_w_crossentropy.pth\", map_location=torch.device('cuda'))\n",
    "resnet.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DL\\env\\Lib\\site-packages\\torchvision\\models\\inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inception = models.inception_v3()\n",
    "num_features = inception.fc.in_features\n",
    "inception.fc = nn.Linear(num_features,4)\n",
    "\n",
    "num_features_aux = inception.AuxLogits.fc.in_features\n",
    "inception.AuxLogits.fc = nn.Linear(num_features_aux, 4)\n",
    "\n",
    "checkpoint = torch.load(\"checkpoints\\\\best_inception_v3_w_crossentropy.pth\", map_location=torch.device('cuda'))\n",
    "inception.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DL\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "vit = timm.create_model('vit_small_patch16_224', pretrained=False)\n",
    "\n",
    "# Modify the final layer to match the number of classes (4 in this case)\n",
    "num_features = vit.head.in_features\n",
    "vit.head = nn.Linear(num_features, 4)\n",
    "checkpoint = torch.load(\"checkpoints\\\\best_vit_small_patch16_224_w_crossentropy.pth\", map_location=torch.device('cuda'))\n",
    "vit.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4862   76   55   11]\n",
      " [ 171  409    1    0]\n",
      " [  46    1  161    0]\n",
      " [   7    1    0   37]]\n",
      "[0.9716227  0.70395869 0.77403846 0.82222222]\n",
      "Test Accuracy: 0.9368, Test F1: 0.8207, Test Precision: 0.8271, Test Recall: 0.8180\n",
      "[[4943   43   17    1]\n",
      " [ 445  136    0    0]\n",
      " [ 124   35   49    0]\n",
      " [  30    3    3    9]]\n",
      "[0.98780975 0.23407917 0.23557692 0.2       ]\n",
      "Test Accuracy: 0.8799, Test F1: 0.4898, Test Precision: 0.7822, Test Recall: 0.4144\n",
      "[[4855   96   44    9]\n",
      " [ 147  433    1    0]\n",
      " [  33    1  174    0]\n",
      " [   6    1    0   38]]\n",
      "[0.97022382 0.74526678 0.83653846 0.84444444]\n",
      "Test Accuracy: 0.9421, Test F1: 0.8466, Test Precision: 0.8454, Test Recall: 0.8491\n"
     ]
    }
   ],
   "source": [
    "models = [resnet, inception]\n",
    "device = torch.device(\"cuda\")\n",
    "weights = [0.6, 0.4]\n",
    "evaluate_ensemble(models, test_dataloader, device, method='averaging')\n",
    "evaluate_ensemble(models, test_dataloader, device, method='max_voting')\n",
    "evaluate_ensemble(models, test_dataloader, device, method='weighted_averaging', weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DL\\env\\Lib\\site-packages\\timm\\models\\vision_transformer.py:86: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  x = F.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4800  134   61    9]\n",
      " [ 113  468    0    0]\n",
      " [  21    0  187    0]\n",
      " [   7    1    0   37]]\n",
      "[0.95923261 0.80550775 0.89903846 0.82222222]\n",
      "Test Accuracy: 0.9407, Test F1: 0.8473, Test Precision: 0.8265, Test Recall: 0.8715\n",
      "[[4970   34    0    0]\n",
      " [ 513   68    0    0]\n",
      " [ 191   17    0    0]\n",
      " [  45    0    0    0]]\n",
      "[0.99320544 0.11703959 0.         0.        ]\n",
      "Test Accuracy: 0.8630, Test F1: 0.2803, Test Precision: 0.3601, Test Recall: 0.2776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DL\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4787  142   66    9]\n",
      " [ 107  474    0    0]\n",
      " [  20    0  187    1]\n",
      " [   6    1    0   38]]\n",
      "[0.95663469 0.81583477 0.89903846 0.84444444]\n",
      "Test Accuracy: 0.9397, Test F1: 0.8461, Test Precision: 0.8180, Test Recall: 0.8790\n"
     ]
    }
   ],
   "source": [
    "models = [resnet, vit]\n",
    "device = torch.device(\"cuda\")\n",
    "weights = [0.6, 0.4]\n",
    "evaluate_ensemble(models, test_dataloader, device, method='averaging')\n",
    "evaluate_ensemble(models, test_dataloader, device, method='max_voting')\n",
    "evaluate_ensemble(models, test_dataloader, device, method='weighted_averaging', weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4175  322  463   44]\n",
      " [ 458   61   57    5]\n",
      " [ 133   30   44    1]\n",
      " [  32    2    4    7]]\n",
      "[0.83433253 0.10499139 0.21153846 0.15555556]\n",
      "Test Accuracy: 0.7343, Test F1: 0.3063, Test Precision: 0.3044, Test Recall: 0.3266\n",
      "[[4963   41    0    0]\n",
      " [ 575    6    0    0]\n",
      " [ 199    9    0    0]\n",
      " [  45    0    0    0]]\n",
      "[0.99180655 0.01032702 0.         0.        ]\n",
      "Test Accuracy: 0.8511, Test F1: 0.2348, Test Precision: 0.2414, Test Recall: 0.2505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DL\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "models = [vit, inception]\n",
    "device = torch.device(\"cuda\")\n",
    "evaluate_ensemble(models, test_dataloader, device, method='averaging')\n",
    "evaluate_ensemble(models, test_dataloader, device, method='max_voting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4888   65   44    7]\n",
      " [ 183  397    1    0]\n",
      " [  52    1  155    0]\n",
      " [  13    0    0   32]]\n",
      "[0.97681855 0.68330465 0.74519231 0.71111111]\n",
      "Test Accuracy: 0.9373, Test F1: 0.8116, Test Precision: 0.8512, Test Recall: 0.7791\n",
      "[[4921   67   15    1]\n",
      " [ 449  132    0    0]\n",
      " [ 159    4   45    0]\n",
      " [  36    0    0    9]]\n",
      "[0.98341327 0.22719449 0.21634615 0.2       ]\n",
      "Test Accuracy: 0.8748, Test F1: 0.4828, Test Precision: 0.7961, Test Recall: 0.4067\n",
      "[[4870   85   41    8]\n",
      " [ 147  433    1    0]\n",
      " [  36    1  171    0]\n",
      " [   7    1    0   37]]\n",
      "[0.97322142 0.74526678 0.82211538 0.82222222]\n",
      "Test Accuracy: 0.9440, Test F1: 0.8472, Test Precision: 0.8550, Test Recall: 0.8407\n"
     ]
    }
   ],
   "source": [
    "models = [resnet, vit, inception]\n",
    "device = torch.device(\"cuda\")\n",
    "weights = [0.44, 0.28, 0.28]\n",
    "evaluate_ensemble(models, test_dataloader, device, method='averaging')\n",
    "evaluate_ensemble(models, test_dataloader, device, method='max_voting')\n",
    "evaluate_ensemble(models, test_dataloader, device, method='weighted_averaging', weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4888   65   44    7]\n",
      " [ 183  397    1    0]\n",
      " [  52    1  155    0]\n",
      " [  13    0    0   32]]\n",
      "[0.97681855 0.68330465 0.74519231 0.71111111]\n",
      "Test Accuracy: 0.9373, Test F1: 0.8116, Test Precision: 0.8512, Test Recall: 0.7791\n",
      "[[4921   67   15    1]\n",
      " [ 449  132    0    0]\n",
      " [ 159    4   45    0]\n",
      " [  36    0    0    9]]\n",
      "[0.98341327 0.22719449 0.21634615 0.2       ]\n",
      "Test Accuracy: 0.8748, Test F1: 0.4828, Test Precision: 0.7961, Test Recall: 0.4067\n",
      "[[4870   85   41    8]\n",
      " [ 147  433    1    0]\n",
      " [  36    1  171    0]\n",
      " [   7    1    0   37]]\n",
      "[0.97322142 0.74526678 0.82211538 0.82222222]\n",
      "Test Accuracy: 0.9440, Test F1: 0.8472, Test Precision: 0.8550, Test Recall: 0.8407\n"
     ]
    }
   ],
   "source": [
    "models = [resnet, vit, inception]\n",
    "device = torch.device(\"cuda\")\n",
    "weights = [0.44, 0.28, 0.28]\n",
    "evaluate_ensemble(models, test_dataloader, device, method='averaging')\n",
    "evaluate_ensemble(models, test_dataloader, device, method='max_voting')\n",
    "evaluate_ensemble(models, test_dataloader, device, method='weighted_averaging', weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Ensemble Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class UNet_with_ResNet_Blocks(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(UNet_with_ResNet_Blocks, self).__init__()\n",
    "        self.encoder1 = ResNetBlock(3, 64)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.encoder2 = ResNetBlock(64, 128)\n",
    "        self.bottleneck = ResNetBlock(128, 256)\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder2 = ResNetBlock(256, 128)\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder1 = ResNetBlock(128, 64)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.encoder1(x)\n",
    "        x = self.pool(enc1)\n",
    "        enc2 = self.encoder2(x)\n",
    "        x = self.pool(enc2)\n",
    "        x = self.bottleneck(x)\n",
    "        x = self.upconv2(x)\n",
    "        x = torch.cat((x, enc2), dim=1)\n",
    "        x = self.decoder2(x)\n",
    "        x = self.upconv1(x)\n",
    "        x = torch.cat((x, enc1), dim=1)\n",
    "        x = self.decoder1(x)\n",
    "\n",
    "        # Classification\n",
    "        x = self.avg_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class VGG_Attention(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(VGG_Attention, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            AttentionLayer(64),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            AttentionLayer(128),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            AttentionLayer(256),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            AttentionLayer(512),\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            AttentionLayer(512),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, reduction_ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels // reduction_ratio, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels // reduction_ratio, in_channels, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size // 2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        out = torch.cat([avg_out, max_out], dim=1)\n",
    "        out = self.conv(out)\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_channels, reduction_ratio=16, kernel_size=7):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.channel_attention = ChannelAttention(in_channels, reduction_ratio)\n",
    "        self.spatial_attention = SpatialAttention(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * self.channel_attention(x)\n",
    "        x = x * self.spatial_attention(x)\n",
    "        return x\n",
    "\n",
    "class CBAMResNet(nn.Module):\n",
    "    def __init__(self, model=models.resnet50(), num_classes=4, in_channels_ls = [256, 512, 1024, 2048]):\n",
    "        super(CBAMResNet, self).__init__()\n",
    "        self.base_model = model\n",
    "        self.cbam1 = CBAM(in_channels=in_channels_ls[0])\n",
    "        self.cbam2 = CBAM(in_channels=in_channels_ls[1])\n",
    "        self.cbam3 = CBAM(in_channels=in_channels_ls[2])\n",
    "        self.cbam4 = CBAM(in_channels=in_channels_ls[3])\n",
    "\n",
    "        num_features = self.base_model.fc.in_features\n",
    "        self.base_model.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model.conv1(x)\n",
    "        x = self.base_model.bn1(x)\n",
    "        x = self.base_model.relu(x)\n",
    "        x = self.base_model.maxpool(x)\n",
    "\n",
    "        x = self.base_model.layer1(x)\n",
    "        x = self.cbam1(x)\n",
    "\n",
    "        x = self.base_model.layer2(x)\n",
    "        x = self.cbam2(x)\n",
    "\n",
    "        x = self.base_model.layer3(x)\n",
    "        x = self.cbam3(x)\n",
    "\n",
    "        x = self.base_model.layer4(x)\n",
    "        x = self.cbam4(x)\n",
    "\n",
    "        x = self.base_model.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.base_model.fc(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        super(SeparableConv2d, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, groups=in_channels)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, 1, 1, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "class XceptionInceptionModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(XceptionInceptionModule, self).__init__()\n",
    "        self.branch1 = SeparableConv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels // 2, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            SeparableConv2d(out_channels // 2, out_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "        # Removed one branch for simplification\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        branch4 = self.branch4(x)\n",
    "        return torch.cat([branch1, branch2, branch4], 1)\n",
    "\n",
    "class XceptionInceptionNet(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(XceptionInceptionNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.xception_inception = XceptionInceptionModule(32, 64)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(64 * 3, num_classes)  # Adjusted for the reduced number of branches\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.xception_inception(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_attention = VGG_Attention(num_classes=4)\n",
    "checkpoint = torch.load(\"checkpoints\\\\best_VGG_Attention.pth\", map_location=torch.device('cuda'))\n",
    "vgg_attention.load_state_dict(checkpoint)\n",
    "\n",
    "cbam18 = CBAMResNet(num_classes=4, model=models.resnet18(), in_channels_ls = [64, 128, 256, 512])\n",
    "checkpoint = torch.load(\"checkpoints\\\\best_CBAMResNet.pth\", map_location=torch.device('cuda'))\n",
    "cbam18.load_state_dict(checkpoint)\n",
    "\n",
    "unet_resnet = UNet_with_ResNet_Blocks(num_classes=4)\n",
    "checkpoint = torch.load(\"checkpoints\\\\best_UNet_with_ResNet_Blocks_w_crossentropy.pth\", map_location=torch.device('cuda'))\n",
    "unet_resnet.load_state_dict(checkpoint)\n",
    "\n",
    "xception = XceptionInceptionNet(num_classes=4)\n",
    "checkpoint = torch.load(\"checkpoints\\\\best_XceptionInceptionNet.pth\", map_location=torch.device('cuda'))\n",
    "xception.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4233  709   51   11]\n",
      " [ 113  465    3    0]\n",
      " [  33    4  171    0]\n",
      " [   1    1    0   43]]\n",
      "[0.84592326 0.80034423 0.82211538 0.95555556]\n",
      "Test Accuracy: 0.8414, Test F1: 0.7723, Test Precision: 0.7293, Test Recall: 0.8560\n",
      "[[5004    0    0    0]\n",
      " [ 581    0    0    0]\n",
      " [ 208    0    0    0]\n",
      " [  45    0    0    0]]\n",
      "[1. 0. 0. 0.]\n",
      "Test Accuracy: 0.8571, Test F1: 0.2308, Test Precision: 0.2143, Test Recall: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DL\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "models = [vgg_attention, cbam18]\n",
    "device = torch.device(\"cuda\")\n",
    "weights = [0.23, 0,77]\n",
    "evaluate_ensemble(models, test_dataloader, device, method='averaging')\n",
    "evaluate_ensemble(models, test_dataloader, device, method='weighted_averaging', weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3916 1067   16    5]\n",
      " [ 350  229    2    0]\n",
      " [ 137   65    5    1]\n",
      " [  31    8    3    3]]\n",
      "[0.78257394 0.39414802 0.02403846 0.06666667]\n",
      "Test Accuracy: 0.7114, Test F1: 0.3046, Test Precision: 0.3940, Test Recall: 0.3169\n",
      "[[3806 1152   37    9]\n",
      " [ 333  240    7    1]\n",
      " [ 126   66   14    2]\n",
      " [  22   10    6    7]]\n",
      "[0.76059153 0.4130809  0.06730769 0.15555556]\n",
      "Test Accuracy: 0.6966, Test F1: 0.3438, Test Precision: 0.4096, Test Recall: 0.3491\n"
     ]
    }
   ],
   "source": [
    "models = [vgg_attention, unet_resnet]\n",
    "device = torch.device(\"cuda\")\n",
    "weights = [0.37, 0.63]\n",
    "evaluate_ensemble(models, test_dataloader, device, method='averaging')\n",
    "evaluate_ensemble(models, test_dataloader, device, method='weighted_averaging', weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4222  708   60   14]\n",
      " [ 118  460    3    0]\n",
      " [  32    3  173    0]\n",
      " [   1    1    0   43]]\n",
      "[0.84372502 0.79173838 0.83173077 0.95555556]\n",
      "Test Accuracy: 0.8390, Test F1: 0.7619, Test Precision: 0.7113, Test Recall: 0.8557\n",
      "[[4177  727   80   20]\n",
      " [ 110  465    5    1]\n",
      " [  26    4  178    0]\n",
      " [   1    1    0   43]]\n",
      "[0.83473221 0.80034423 0.85576923 0.95555556]\n",
      "Test Accuracy: 0.8330, Test F1: 0.7411, Test Precision: 0.6763, Test Recall: 0.8616\n"
     ]
    }
   ],
   "source": [
    "models = [xception, cbam18]\n",
    "weights = [0.23, 0.77]\n",
    "device = torch.device(\"cuda\")\n",
    "evaluate_ensemble(models, test_dataloader, device, method='averaging')\n",
    "evaluate_ensemble(models, test_dataloader, device, method='weighted_averaging', weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4125  838   36    5]\n",
      " [ 390  185    6    0]\n",
      " [ 143   50   14    1]\n",
      " [  34    4    4    3]]\n",
      "[0.82434053 0.31841652 0.06730769 0.06666667]\n",
      "Test Accuracy: 0.7412, Test F1: 0.3224, Test Precision: 0.4044, Test Recall: 0.3192\n",
      "[[4278  704   20    2]\n",
      " [ 429  150    2    0]\n",
      " [ 165   37    6    0]\n",
      " [  35    4    3    3]]\n",
      "[0.85491607 0.25817556 0.02884615 0.06666667]\n",
      "Test Accuracy: 0.7600, Test F1: 0.3092, Test Precision: 0.4582, Test Recall: 0.3022\n"
     ]
    }
   ],
   "source": [
    "models = [unet_resnet, xception]\n",
    "device = torch.device(\"cuda\")\n",
    "weights = [0.38, 0.62]\n",
    "evaluate_ensemble(models, test_dataloader, device, method='averaging')\n",
    "evaluate_ensemble(models, test_dataloader, device, method='weighted_averaging', weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4224  734   41    5]\n",
      " [ 124  455    2    0]\n",
      " [  38    3  167    0]\n",
      " [   3    2    0   40]]\n",
      "[0.8441247  0.78313253 0.80288462 0.88888889]\n",
      "Test Accuracy: 0.8369, Test F1: 0.7750, Test Precision: 0.7569, Test Recall: 0.8298\n",
      "[[4575  418   11    0]\n",
      " [ 361  220    0    0]\n",
      " [ 176    2   30    0]\n",
      " [  32    1    0   12]]\n",
      "[0.91426859 0.37865749 0.14423077 0.26666667]\n",
      "Test Accuracy: 0.8285, Test F1: 0.4809, Test Precision: 0.7411, Test Recall: 0.4260\n",
      "[[4190  770   40    4]\n",
      " [ 126  454    1    0]\n",
      " [  41    5  162    0]\n",
      " [   3    2    0   40]]\n",
      "[0.83733014 0.78141136 0.77884615 0.88888889]\n",
      "Test Accuracy: 0.8301, Test F1: 0.7708, Test Precision: 0.7592, Test Recall: 0.8216\n"
     ]
    }
   ],
   "source": [
    "models = [vgg_attention, unet_resnet, cbam18]\n",
    "device = torch.device(\"cuda\")\n",
    "weights = [0.159, 0.539, 0.302]\n",
    "evaluate_ensemble(models, test_dataloader, device, method='averaging')\n",
    "evaluate_ensemble(models, test_dataloader, device, method='max_voting')\n",
    "evaluate_ensemble(models, test_dataloader, device, method='weighted_averaging', weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4235  718   46    5]\n",
      " [ 131  448    2    0]\n",
      " [  37    3  168    0]\n",
      " [   3    2    0   40]]\n",
      "[0.84632294 0.77108434 0.80769231 0.88888889]\n",
      "Test Accuracy: 0.8378, Test F1: 0.7732, Test Precision: 0.7526, Test Recall: 0.8285\n",
      "[[4406  587   11    0]\n",
      " [ 349  232    0    0]\n",
      " [ 164   14   30    0]\n",
      " [  32    1    0   12]]\n",
      "[0.8804956  0.39931153 0.14423077 0.26666667]\n",
      "Test Accuracy: 0.8016, Test F1: 0.4688, Test Precision: 0.7250, Test Recall: 0.4227\n",
      "[[4202  759   39    4]\n",
      " [ 135  445    1    0]\n",
      " [  42    5  161    0]\n",
      " [   4    2    0   39]]\n",
      "[0.83972822 0.76592083 0.77403846 0.86666667]\n",
      "Test Accuracy: 0.8303, Test F1: 0.7664, Test Precision: 0.7585, Test Recall: 0.8116\n"
     ]
    }
   ],
   "source": [
    "models = [xception, unet_resnet, cbam18 ]\n",
    "device = torch.device(\"cuda\")\n",
    "weights = [0.167, 0.560, 0.273]\n",
    "evaluate_ensemble(models, test_dataloader, device, method='averaging')\n",
    "evaluate_ensemble(models, test_dataloader, device, method='max_voting')\n",
    "evaluate_ensemble(models, test_dataloader, device, method='weighted_averaging', weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

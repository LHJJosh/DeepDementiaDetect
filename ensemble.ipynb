{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.ndimage import zoom\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torchsummary import summary\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix\n",
    "\n",
    "torch.cuda.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def fix_seed(seed):\n",
    "    '''\n",
    "    Args : \n",
    "        seed : fix the seed\n",
    "    Function which allows to fix all the seed and get reproducible results\n",
    "    '''\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    os.environ['OMP_NUM_THREADS'] = '1'\n",
    "    os.environ['MKL_NUM_THREADS'] = '1'\n",
    "    torch.set_num_threads(1)\n",
    "\n",
    "fix_seed(42)\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the right data:\n",
    "\n",
    "The full dataset has been processed from it's original 350GB+ MRI dataset. The original csv file is filtered to include only 1.5T or 3.0T scans as they contain the most comprehensive sets of MRI scans of differing types (T1-weighted, T2, Bold, etc). \n",
    "\n",
    "The clinical dementia rating (cdr) with values (0, 0.5, 1, 2) represent 0 = absent; 0.5 = questionable; 1= present, but mild; 2 = moderate (reference: https://www.sciencedirect.com/topics/neuroscience/clinical-dementia-rating). They have been remapped from (0, 0.5, 1, 2) to (0, 1, 2, 3) due to issues processing it with Torch dataloader, leading to a missing class for 0.5 cdr.\n",
    "\n",
    "Based on each MRI session value (Label), if there are multiple values for the cdr then the scans are excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1379\n",
      "['d0129', 'd2430', 'd3132']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       d0129\n",
       "2       d2430\n",
       "3       d3132\n",
       "12      d0371\n",
       "15      d2340\n",
       "        ...  \n",
       "6193    d0148\n",
       "6194    d2526\n",
       "6195    d1566\n",
       "6216    d1717\n",
       "6217    d0407\n",
       "Name: file_tag, Length: 1379, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagnostic_file = glob('diagnosis.csv')[0]\n",
    "\n",
    "diagnostic_df = pd.read_csv(diagnostic_file)\n",
    "\n",
    "diagnostic_df = diagnostic_df[(diagnostic_df.Scanner == '3.0T') | (diagnostic_df.Scanner == '1.5T')]\n",
    "\n",
    "multiple_values = diagnostic_df.groupby('Label').filter(lambda group: group['cdr'].nunique() > 1)['Label'].unique()\n",
    "\n",
    "cdr_map = {0.0: 0, 0.5: 1, 1.0: 2, 2.0: 3}\n",
    "\n",
    "# Filter out rows with these values in column A\n",
    "filtered_diagnostic_df = diagnostic_df[~diagnostic_df['Label'].isin(multiple_values)]\n",
    "filtered_diagnostic_df = filtered_diagnostic_df.drop_duplicates(subset='Label')\n",
    "# filtered_diagnostic_df['cdr'] = filtered_diagnostic_df['cdr'].map(cdr_map)\n",
    "filtered_diagnostic_df['file_tag'] = filtered_diagnostic_df.Label.apply(lambda file_name: file_name[-5:])\n",
    "print(len(filtered_diagnostic_df))\n",
    "\n",
    "valid_mr_scans_ls = filtered_diagnostic_df.file_tag.to_list()\n",
    "print(valid_mr_scans_ls[:3])\n",
    "filtered_diagnostic_df.file_tag\n",
    "\n",
    "# filtered_diagnostic_df.to_csv('filtered_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bids\\\\sub-OAS30001\\\\ses-d0129\\\\anat\\\\sub-OAS30001_ses-d0129_run-01_T1w.nii.gz', 'bids\\\\sub-OAS30001\\\\ses-d0129\\\\anat\\\\sub-OAS30001_ses-d0129_run-02_T1w.nii.gz']\n"
     ]
    }
   ],
   "source": [
    "data_path = 'bids/' ## Change as needed\n",
    "\n",
    "def recursive_glob_with_filter(directory, filter_list):\n",
    "    # Use recursive glob pattern to find all files\n",
    "    all_files = glob(directory + '/**/*T1*nii.gz', recursive=True)\n",
    "    \n",
    "    # Filter files based on the presence of strings from filter_list in the file path\n",
    "    filtered_files = [file for file in all_files if any(filter_str in file for filter_str in filter_list)]\n",
    "    \n",
    "    return filtered_files\n",
    "\n",
    "file_ls = recursive_glob_with_filter(data_path, valid_mr_scans_ls)\n",
    "print(file_ls[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0                                          file_path  label\n",
      "0              0  D:/DL/oasis-scripts/download_scans/bids\\sub-OA...    0.0\n",
      "1              1  D:/DL/oasis-scripts/download_scans/bids\\sub-OA...    0.0\n",
      "2              2  D:/DL/oasis-scripts/download_scans/bids\\sub-OA...    0.0\n",
      "3              3  D:/DL/oasis-scripts/download_scans/bids\\sub-OA...    0.0\n",
      "4              4  D:/DL/oasis-scripts/download_scans/bids\\sub-OA...    0.0\n",
      "...          ...                                                ...    ...\n",
      "2991        2991  D:/DL/oasis-scripts/download_scans/bids\\sub-OA...    0.0\n",
      "2992        2992  D:/DL/oasis-scripts/download_scans/bids\\sub-OA...    1.0\n",
      "2993        2993  D:/DL/oasis-scripts/download_scans/bids\\sub-OA...    0.0\n",
      "2994        2994  D:/DL/oasis-scripts/download_scans/bids\\sub-OA...    0.5\n",
      "2995        2995  D:/DL/oasis-scripts/download_scans/bids\\sub-OA...    0.0\n",
      "\n",
      "[2996 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Function to extract tag from file path\n",
    "def extract_tag_from_file_path(file_path):\n",
    "    file_path = file_path.replace(data_path[:-1], '')\n",
    "    return file_path.split('\\\\')[2][-5:]\n",
    "\n",
    "try:\n",
    "    data_loader_df = pd.read_csv('data_loader_df.csv')\n",
    "except:\n",
    "    # Create a new DataFrame to store the results\n",
    "    data_loader_df = pd.DataFrame(columns=['file_path', 'label'])\n",
    "\n",
    "    # Iterate over the file list and find the corresponding CDR value\n",
    "    for file_path in file_ls:\n",
    "        tag = extract_tag_from_file_path(file_path)\n",
    "        cdr_value = filtered_diagnostic_df.loc[filtered_diagnostic_df['file_tag'] == tag, 'cdr'].values\n",
    "        if cdr_value.size > 0:  # Check if cdr_value is not empty\n",
    "            new_row = pd.DataFrame({'file_path': [file_path], 'label': [cdr_value[0]]})\n",
    "            data_loader_df = pd.concat([data_loader_df, new_row], ignore_index=True)\n",
    "            data_loader_df.to_csv('data_loader_df.csv')\n",
    "\n",
    "print(data_loader_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is to extract 2D slices from 3D volumetric MRI scans. It iterates through the T1-weighted scans and slices between indexes 100-160, choosing a slice for every 3 slices. If the scan has less then 160 slices, then the middle slice will be taken of which there is only 1. Eventually there should be a dataset of 58,376. \n",
    "\n",
    "JPG is the saved file format due to its smaller size. NPY was tried but was 840GB and hence rejected.\n",
    "\n",
    "**Run only once (roughly 7-20 mins depending on CPU).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to an extremely imbalanced dataset (Label 0.0: 50216; Label 1.0: 5748; Label 2.0: 1971; Label 3.0: 441), weights will be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0.0: 50216\n",
      "Label 1.0: 5748\n",
      "Label 2.0: 1971\n",
      "Label 3.0: 441\n",
      "{0.0: 1.1624980086028358, 1.0: 10.155880306193458, 2.0: 29.617453069507864, 3.0: 132.3718820861678}\n",
      "cuda\n",
      "tensor([  1.1625,  10.1559,  29.6175, 132.3719])\n",
      "7.153921568627451\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "save_dir = data_path[:-5] + \"preprocessed_images\"\n",
    "# Dictionary to store the count of each label\n",
    "class_counts = defaultdict(int)\n",
    "\n",
    "# List to store file paths and labels\n",
    "file_paths_labels = []\n",
    "\n",
    "# Iterate over all files in the save directory\n",
    "for filename in os.listdir(save_dir):\n",
    "    if filename.startswith(\"label_\") and filename.endswith(\".jpg\"):\n",
    "        # Extract the label from the filename and convert it to a decimal (float)\n",
    "        label = float(filename.split(\"_\")[1])\n",
    "        # Increment the count for this label\n",
    "        class_counts[label] += 1\n",
    "        # Append the full file path and label to the list\n",
    "        full_path = os.path.join(save_dir, filename)\n",
    "        file_paths_labels.append((full_path, label))\n",
    "\n",
    "dementia_count = 0\n",
    "healthy_count = 0\n",
    "# Print the count of each label\n",
    "for label, count in class_counts.items():\n",
    "    print(f\"Label {label}: {count}\")\n",
    "    if label >0:\n",
    "        dementia_count+=count\n",
    "    else:\n",
    "        healthy_count+=count\n",
    "# Create a DataFrame from the list of file paths and labels\n",
    "jpg_data_loader_df = pd.DataFrame(file_paths_labels, columns=['file_path', 'label'])\n",
    "\n",
    "## Print the first few rows of the DataFrame\n",
    "# print(jpg_data_loader_df.head())\n",
    "\n",
    "# Calculate weights: Inverse of the frequency seems like a simple choice\n",
    "total_count = sum(class_counts.values())\n",
    "weights = {k: total_count / v for k, v in class_counts.items()}\n",
    "print(weights)\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# Convert weights to a tensor, ensuring the labels are in the correct order\n",
    "weights_tensor = torch.tensor([weights[0], weights[1], weights[2], weights[3]], dtype=torch.float32)\n",
    "\n",
    "print(weights_tensor)\n",
    "weights_tensor = weights_tensor.to(device)\n",
    "jpg_data_loader_df.to_csv('jpg_data_loader_df.csv')\n",
    "\n",
    "all_dementia_weight = (healthy_count+dementia_count)/dementia_count\n",
    "# all_dementia_weight = 3\n",
    "print(all_dementia_weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "Functions for training the model. Includes NiftiDataset, HierarchicalCrossEntropyLoss, load_model, train_model and evaluate_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0, 1.0, 2.0, 3.0}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "class NiftiDataset(Dataset):\n",
    "    def __init__(self, dataframe, preprocessed_dir):\n",
    "        self.dataframe = dataframe\n",
    "        self.preprocessed_dir = preprocessed_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        label = row['label']\n",
    "        img_path = row['file_path']  # Use the exact path from the dataframe\n",
    "        scan = cv2.imread(img_path)\n",
    "        # scan = cv2.resize(scan, (224,224), interpolation=cv2.INTER_LINEAR)\n",
    "        scan = scan / 255.0  # Normalize the image to [0, 1]\n",
    "        scan_tensor = torch.from_numpy(scan).float()\n",
    "        \n",
    "        # If the image is grayscale, we use unsqueeze to add the channel dimension\n",
    "        if len(scan.shape) == 2:\n",
    "            scan_tensor = scan_tensor.unsqueeze(0)  # Add channel dimension for grayscale image\n",
    "        else:\n",
    "            scan_tensor = scan_tensor.permute(2, 0, 1)  # Rearrange dimensions for color image\n",
    "        \n",
    "        label_tensor = torch.tensor(label).long()\n",
    "\n",
    "        return scan_tensor, label_tensor\n",
    "\n",
    "dataset = NiftiDataset(jpg_data_loader_df, preprocessed_dir=save_dir)\n",
    "\n",
    "# Create a list of labels for stratified splitting\n",
    "labels = jpg_data_loader_df.iloc[:, 1].values\n",
    "myset = set(labels)\n",
    "print(myset)\n",
    "\n",
    "# Define the ratios for splitting\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "# Calculate the sizes for each split\n",
    "total_size = len(dataset)\n",
    "train_size = int(total_size * train_ratio)\n",
    "val_size = int(total_size * val_ratio)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_dataset, val_test_dataset = random_split(dataset, [train_size, val_size + test_size])\n",
    "val_dataset, test_dataset = random_split(val_test_dataset, [val_size, test_size])\n",
    "\n",
    "# Create dataloaders for each split\n",
    "batch_size = 32  # Adjust the batch size as needed\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Function to count labels in a dataset\n",
    "def count_labels(dataset):\n",
    "    label_counts = {}\n",
    "    for _, label_tensor in dataset:\n",
    "        label = label_tensor.item()\n",
    "        if label in label_counts:\n",
    "            label_counts[label] += 1\n",
    "        else:\n",
    "            label_counts[label] = 1\n",
    "    return label_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def evaluate_ensemble(models, test_dataloader, device, method='averaging', weights=None):\n",
    "    predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    if method in ['weighted_ensemble', 'weighted_max_voting']:\n",
    "        if weights is None:\n",
    "            # Initialize weights with equal values\n",
    "            weights = [1.0 / len(models)] * len(models)\n",
    "        elif len(weights) != len(models):\n",
    "            raise ValueError(\"Number of weights must be equal to the number of models.\")\n",
    "\n",
    "    for inputs, labels in test_dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        if method == 'averaging':\n",
    "            ensemble_outputs = []\n",
    "            for i, model in enumerate(models):\n",
    "                model.cuda()\n",
    "                model.eval()\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                if hasattr(outputs, 'aux_logits'):\n",
    "                    outputs = outputs.logits\n",
    "\n",
    "                ensemble_outputs.append(outputs)\n",
    "\n",
    "            ensemble_outputs = torch.stack(ensemble_outputs, dim=0)\n",
    "            ensemble_mean = torch.mean(ensemble_outputs, dim=0)\n",
    "            _, predicted = torch.max(ensemble_mean, dim=1)\n",
    "        elif method == 'max_voting':\n",
    "            ensemble_votes = torch.zeros((len(models), len(inputs)), dtype=torch.long).to(device)\n",
    "            for i, model in enumerate(models):\n",
    "                model.cuda()\n",
    "                model.eval()\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                if hasattr(outputs, 'aux_logits'):\n",
    "                    outputs = outputs.logits\n",
    "\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                ensemble_votes[i] = predicted\n",
    "\n",
    "            predicted, _ = torch.mode(ensemble_votes, dim=0)\n",
    "        elif method == 'weighted_averaging':\n",
    "            ensemble_outputs = []\n",
    "            for i, model in enumerate(models):\n",
    "                model.cuda()\n",
    "                model.eval()\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                if hasattr(outputs, 'aux_logits'):\n",
    "                    outputs = outputs.logits\n",
    "\n",
    "                outputs *= weights[i]\n",
    "                ensemble_outputs.append(outputs)\n",
    "\n",
    "            ensemble_mean = torch.mean(torch.stack(ensemble_outputs, dim=0), dim=0)\n",
    "            _, predicted = torch.max(ensemble_mean, dim=1)\n",
    "            \n",
    "\n",
    "        predictions.extend(predicted.tolist())\n",
    "        all_labels.extend(labels.tolist())\n",
    "\n",
    "    cm = confusion_matrix(all_labels, predictions)\n",
    "    print(cm)\n",
    "    class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "    print(class_accuracies)\n",
    "    \n",
    "    accuracy = torch.sum(torch.tensor(predictions) == torch.tensor(all_labels)).item() / len(predictions)\n",
    "    f1 = f1_score(all_labels, predictions, average='macro')\n",
    "    precision = precision_score(all_labels, predictions, average='macro')\n",
    "    recall = recall_score(all_labels, predictions, average='macro')\n",
    "    \n",
    "    print(f'Test Accuracy: {accuracy:.4f}, Test F1: {f1:.4f}, Test Precision: {precision:.4f}, Test Recall: {recall:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load models from checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet = models.resnet18()\n",
    "num_features = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(num_features, 4)\n",
    "\n",
    "checkpoint = torch.load(\"checkpoints\\\\best_resnet18_w_crossentropy.pth\", map_location=torch.device('cuda'))\n",
    "resnet.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DL\\env\\Lib\\site-packages\\torchvision\\models\\inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inception = models.inception_v3()\n",
    "num_features = inception.fc.in_features\n",
    "inception.fc = nn.Linear(num_features,4)\n",
    "\n",
    "num_features_aux = inception.AuxLogits.fc.in_features\n",
    "inception.AuxLogits.fc = nn.Linear(num_features_aux, 4)\n",
    "\n",
    "checkpoint = torch.load(\"checkpoints\\\\best_inception_v3_w_crossentropy.pth\", map_location=torch.device('cuda'))\n",
    "inception.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DL\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "vit = timm.create_model('vit_small_patch16_224', pretrained=False)\n",
    "\n",
    "# Modify the final layer to match the number of classes (4 in this case)\n",
    "num_features = vit.head.in_features\n",
    "vit.head = nn.Linear(num_features, 4)\n",
    "checkpoint = torch.load(\"checkpoints\\\\best_vit_small_patch16_224_w_crossentropy.pth\", map_location=torch.device('cuda'))\n",
    "vit.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Ensemble Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class UNet_with_ResNet_Blocks(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(UNet_with_ResNet_Blocks, self).__init__()\n",
    "        self.encoder1 = ResNetBlock(3, 64)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.encoder2 = ResNetBlock(64, 128)\n",
    "        self.bottleneck = ResNetBlock(128, 256)\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder2 = ResNetBlock(256, 128)\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder1 = ResNetBlock(128, 64)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.encoder1(x)\n",
    "        x = self.pool(enc1)\n",
    "        enc2 = self.encoder2(x)\n",
    "        x = self.pool(enc2)\n",
    "        x = self.bottleneck(x)\n",
    "        x = self.upconv2(x)\n",
    "        x = torch.cat((x, enc2), dim=1)\n",
    "        x = self.decoder2(x)\n",
    "        x = self.upconv1(x)\n",
    "        x = torch.cat((x, enc1), dim=1)\n",
    "        x = self.decoder1(x)\n",
    "\n",
    "        # Classification\n",
    "        x = self.avg_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class VGG_Attention(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(VGG_Attention, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            AttentionLayer(64),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            AttentionLayer(128),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            AttentionLayer(256),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            AttentionLayer(512),\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            AttentionLayer(512),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, reduction_ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels // reduction_ratio, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels // reduction_ratio, in_channels, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size // 2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        out = torch.cat([avg_out, max_out], dim=1)\n",
    "        out = self.conv(out)\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_channels, reduction_ratio=16, kernel_size=7):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.channel_attention = ChannelAttention(in_channels, reduction_ratio)\n",
    "        self.spatial_attention = SpatialAttention(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * self.channel_attention(x)\n",
    "        x = x * self.spatial_attention(x)\n",
    "        return x\n",
    "\n",
    "class CBAMResNet(nn.Module):\n",
    "    def __init__(self, model=models.resnet50(), num_classes=4, in_channels_ls = [256, 512, 1024, 2048]):\n",
    "        super(CBAMResNet, self).__init__()\n",
    "        self.base_model = model\n",
    "        self.cbam1 = CBAM(in_channels=in_channels_ls[0])\n",
    "        self.cbam2 = CBAM(in_channels=in_channels_ls[1])\n",
    "        self.cbam3 = CBAM(in_channels=in_channels_ls[2])\n",
    "        self.cbam4 = CBAM(in_channels=in_channels_ls[3])\n",
    "\n",
    "        num_features = self.base_model.fc.in_features\n",
    "        self.base_model.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model.conv1(x)\n",
    "        x = self.base_model.bn1(x)\n",
    "        x = self.base_model.relu(x)\n",
    "        x = self.base_model.maxpool(x)\n",
    "\n",
    "        x = self.base_model.layer1(x)\n",
    "        x = self.cbam1(x)\n",
    "\n",
    "        x = self.base_model.layer2(x)\n",
    "        x = self.cbam2(x)\n",
    "\n",
    "        x = self.base_model.layer3(x)\n",
    "        x = self.cbam3(x)\n",
    "\n",
    "        x = self.base_model.layer4(x)\n",
    "        x = self.cbam4(x)\n",
    "\n",
    "        x = self.base_model.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.base_model.fc(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        super(SeparableConv2d, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, groups=in_channels)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, 1, 1, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "class XceptionInceptionModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(XceptionInceptionModule, self).__init__()\n",
    "        self.branch1 = SeparableConv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels // 2, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            SeparableConv2d(out_channels // 2, out_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "        # Removed one branch for simplification\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        branch4 = self.branch4(x)\n",
    "        return torch.cat([branch1, branch2, branch4], 1)\n",
    "\n",
    "class XceptionInceptionNet(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(XceptionInceptionNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.xception_inception = XceptionInceptionModule(32, 64)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(64 * 3, num_classes)  # Adjusted for the reduced number of branches\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.xception_inception(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_attention = VGG_Attention(num_classes=4)\n",
    "checkpoint = torch.load(\"checkpoints\\\\best_VGG_Attention.pth\", map_location=torch.device('cuda'))\n",
    "vgg_attention.load_state_dict(checkpoint)\n",
    "\n",
    "cbam18 = CBAMResNet(num_classes=4, model=models.resnet18(), in_channels_ls = [64, 128, 256, 512])\n",
    "checkpoint = torch.load(\"checkpoints\\\\best_CBAMResNet.pth\", map_location=torch.device('cuda'))\n",
    "cbam18.load_state_dict(checkpoint)\n",
    "\n",
    "unet_resnet = UNet_with_ResNet_Blocks(num_classes=4)\n",
    "checkpoint = torch.load(\"checkpoints\\\\best_UNet_with_ResNet_Blocks_w_crossentropy.pth\", map_location=torch.device('cuda'))\n",
    "unet_resnet.load_state_dict(checkpoint)\n",
    "\n",
    "xception = XceptionInceptionNet(num_classes=4)\n",
    "checkpoint = torch.load(\"checkpoints\\\\best_XceptionInceptionNet.pth\", map_location=torch.device('cuda'))\n",
    "xception.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4271  675   69   13]\n",
      " [ 117  453    1    0]\n",
      " [  32    6  150    1]\n",
      " [   2    0    0   48]]\n",
      "[0.84944312 0.79334501 0.79365079 0.96      ]\n",
      "Test Accuracy: 0.8431, Test F1: 0.7565, Test Precision: 0.7053, Test Recall: 0.8491\n",
      "[[5028    0    0    0]\n",
      " [ 571    0    0    0]\n",
      " [ 189    0    0    0]\n",
      " [  50    0    0    0]]\n",
      "[1. 0. 0. 0.]\n",
      "Test Accuracy: 0.8613, Test F1: 0.2314, Test Precision: 0.2153, Test Recall: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DL\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "models = [vgg_attention, cbam18]\n",
    "device = torch.device(\"cuda\")\n",
    "weights = [0.23, 0,77]\n",
    "evaluate_ensemble(models, test_dataloader, device, method='averaging')\n",
    "evaluate_ensemble(models, test_dataloader, device, method='weighted_averaging', weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3937 1066   18    7]\n",
      " [ 322  249    0    0]\n",
      " [ 119   63    3    4]\n",
      " [  29    7    4   10]]\n",
      "[0.78301512 0.43607706 0.01587302 0.2       ]\n",
      "Test Accuracy: 0.7193, Test F1: 0.3497, Test Precision: 0.4173, Test Recall: 0.3587\n",
      "[[3821 1150   44   13]\n",
      " [ 312  254    5    0]\n",
      " [ 108   67    8    6]\n",
      " [  25    8    4   13]]\n",
      "[0.75994431 0.44483363 0.04232804 0.26      ]\n",
      "Test Accuracy: 0.7016, Test F1: 0.3628, Test Precision: 0.4012, Test Recall: 0.3768\n"
     ]
    }
   ],
   "source": [
    "models = [vgg_attention, unet_resnet]\n",
    "device = torch.device(\"cuda\")\n",
    "weights = [0.37, 0.63]\n",
    "evaluate_ensemble(models, test_dataloader, device, method='averaging')\n",
    "evaluate_ensemble(models, test_dataloader, device, method='weighted_averaging', weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4268  670   76   14]\n",
      " [ 117  452    2    0]\n",
      " [  31    6  151    1]\n",
      " [   1    0    0   49]]\n",
      "[0.84884646 0.7915937  0.7989418  0.98      ]\n",
      "Test Accuracy: 0.8428, Test F1: 0.7545, Test Precision: 0.6980, Test Recall: 0.8548\n",
      "[[4223  699   89   17]\n",
      " [ 114  454    3    0]\n",
      " [  26    6  156    1]\n",
      " [   1    0    0   49]]\n",
      "[0.83989658 0.79509632 0.82539683 0.98      ]\n",
      "Test Accuracy: 0.8362, Test F1: 0.7439, Test Precision: 0.6799, Test Recall: 0.8601\n"
     ]
    }
   ],
   "source": [
    "models = [xception, cbam18]\n",
    "weights = [0.23, 0.77]\n",
    "device = torch.device(\"cuda\")\n",
    "evaluate_ensemble(models, test_dataloader, device, method='averaging')\n",
    "evaluate_ensemble(models, test_dataloader, device, method='weighted_averaging', weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4172  806   42    8]\n",
      " [ 373  194    4    0]\n",
      " [ 131   44    9    5]\n",
      " [  31    4    4   11]]\n",
      "[0.82975338 0.33975482 0.04761905 0.22      ]\n",
      "Test Accuracy: 0.7513, Test F1: 0.3667, Test Precision: 0.4206, Test Recall: 0.3593\n",
      "[[4355  650   18    5]\n",
      " [ 412  158    1    0]\n",
      " [ 150   35    3    1]\n",
      " [  34    4    4    8]]\n",
      "[0.86614956 0.27670753 0.01587302 0.16      ]\n",
      "Test Accuracy: 0.7749, Test F1: 0.3434, Test Precision: 0.4382, Test Recall: 0.3297\n"
     ]
    }
   ],
   "source": [
    "models = [unet_resnet, xception]\n",
    "device = torch.device(\"cuda\")\n",
    "weights = [0.38, 0.62]\n",
    "evaluate_ensemble(models, test_dataloader, device, method='averaging')\n",
    "evaluate_ensemble(models, test_dataloader, device, method='weighted_averaging', weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4278  699   49    2]\n",
      " [ 122  449    0    0]\n",
      " [  38    7  143    1]\n",
      " [   4    0    0   46]]\n",
      "[0.85083532 0.78633975 0.75661376 0.92      ]\n",
      "Test Accuracy: 0.8421, Test F1: 0.7759, Test Precision: 0.7588, Test Recall: 0.8284\n",
      "[[4646  373    9    0]\n",
      " [ 335  236    0    0]\n",
      " [ 168    4   17    0]\n",
      " [  33    0    0   17]]\n",
      "[0.92402546 0.41330998 0.08994709 0.34      ]\n",
      "Test Accuracy: 0.8421, Test F1: 0.4936, Test Precision: 0.7339, Test Recall: 0.4418\n",
      "[[4259  712   57    0]\n",
      " [ 129  442    0    0]\n",
      " [  43    8  137    1]\n",
      " [   3    0    0   47]]\n",
      "[0.84705648 0.77408056 0.72486772 0.94      ]\n",
      "Test Accuracy: 0.8368, Test F1: 0.7712, Test Precision: 0.7566, Test Recall: 0.8215\n"
     ]
    }
   ],
   "source": [
    "models = [vgg_attention, unet_resnet, cbam18]\n",
    "device = torch.device(\"cuda\")\n",
    "weights = [0.159, 0.539, 0.302]\n",
    "evaluate_ensemble(models, test_dataloader, device, method='averaging')\n",
    "evaluate_ensemble(models, test_dataloader, device, method='max_voting')\n",
    "evaluate_ensemble(models, test_dataloader, device, method='weighted_averaging', weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4278  690   58    2]\n",
      " [ 126  444    1    0]\n",
      " [  37    6  145    1]\n",
      " [   3    0    0   47]]\n",
      "[0.85083532 0.77758319 0.76719577 0.94      ]\n",
      "Test Accuracy: 0.8417, Test F1: 0.7751, Test Precision: 0.7507, Test Recall: 0.8339\n",
      "[[4484  535    9    0]\n",
      " [ 323  248    0    0]\n",
      " [ 156   16   17    0]\n",
      " [  33    0    0   17]]\n",
      "[0.89180589 0.43432574 0.08994709 0.34      ]\n",
      "Test Accuracy: 0.8164, Test F1: 0.4806, Test Precision: 0.7154, Test Recall: 0.4390\n",
      "[[4268  704   56    0]\n",
      " [ 133  438    0    0]\n",
      " [  46    8  134    1]\n",
      " [   4    0    0   46]]\n",
      "[0.84884646 0.76707531 0.70899471 0.92      ]\n",
      "Test Accuracy: 0.8369, Test F1: 0.7663, Test Precision: 0.7559, Test Recall: 0.8112\n"
     ]
    }
   ],
   "source": [
    "models = [xception, unet_resnet, cbam18 ]\n",
    "device = torch.device(\"cuda\")\n",
    "weights = [0.167, 0.560, 0.273]\n",
    "evaluate_ensemble(models, test_dataloader, device, method='averaging')\n",
    "evaluate_ensemble(models, test_dataloader, device, method='max_voting')\n",
    "evaluate_ensemble(models, test_dataloader, device, method='weighted_averaging', weights=weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
